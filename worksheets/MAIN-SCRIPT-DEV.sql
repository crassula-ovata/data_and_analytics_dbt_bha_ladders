use role sysadmin;

create database IF NOT EXISTS DM_LADDERS_TEST;
use database DM_LADDERS_TEST;

create schema IF NOT EXISTS DM;
use schema DM;

create schema IF NOT EXISTS UTIL;
use schema UTIL;

CREATE OR REPLACE file format sf_to_s3_unload_file_format
	type=JSON TIME_FORMAT=AUTO DATE_FORMAT=AUTO COMPRESSION=NONE FILE_EXTENSION='json';
    
CREATE OR REPLACE STAGE s3_json_payload_stage
    url='s3://commcare-snowflake-data-sync/co-carecoordination-test/payload/'
    Storage_integration = s3_int_obj
    file_format = sf_to_s3_unload_file_format;

create or replace TABLE task_log (
	TASK_ID NUMBER(38,0) NOT NULL autoincrement order,
	UUID STRING,
	TYPE STRING,
	SUBTYPE STRING,
	DOMAIN STRING,
	STATUS STRING,
	TASK_START TIMESTAMP_NTZ,
	TASK_END TIMESTAMP_NTZ,
	constraint TASK_PK primary key (TASK_ID)
);

create or replace TABLE execution_log (
	EXECUTION_ID NUMBER(38,0) NOT NULL autoincrement order,
    TASK_ID NUMBER(38,0),
	UUID STRING,
	TYPE STRING,
	SUBTYPE STRING,
	DOMAIN STRING,
	STATUS STRING,
	EXECUTION_START TIMESTAMP_NTZ,
	EXECUTION_END TIMESTAMP_NTZ,
	constraint EXECUTION_PK primary key (EXECUTION_ID)
);

create or replace TABLE message_log (
	MESSAGE_ID NUMBER(38,0) NOT NULL autoincrement order,
    TASK_ID NUMBER(38,0),
    EXECUTION_ID NUMBER(38,0),
	TYPE STRING,
	SUBTYPE STRING,
	MESSAGE VARIANT,
	MESSAGE_TIME TIMESTAMP_NTZ DEFAULT SYSDATE(),
	constraint MESSAGE_PK primary key (MESSAGE_ID)
);

create or replace TABLE SQL_LOGS (
	SQL_LOG_ID NUMBER(38,0) NOT NULL autoincrement order,
	TASK_ID NUMBER(38,0), -- change
    EXECUTION_ID NUMBER(38,0), -- change
	QUERY_ID VARCHAR(16777216),
	QUERY_TEXT VARCHAR(16777216),
	DATABASE_NAME VARCHAR(16777216),
	SCHEMA_NAME VARCHAR(16777216),
	QUERY_TYPE VARCHAR(16777216),
	SESSION_ID NUMBER(38,0),
	USER_NAME VARCHAR(16777216),
	ROLE_NAME VARCHAR(16777216),
	WAREHOUSE_NAME VARCHAR(16777216),
	WAREHOUSE_SIZE VARCHAR(16777216),
	WAREHOUSE_TYPE VARCHAR(16777216),
	CLUSTER_NUMBER NUMBER(38,0),
	QUERY_TAG VARCHAR(16777216),
	EXECUTION_STATUS VARCHAR(16777216),
	ERROR_CODE NUMBER(38,0),
	ERROR_MESSAGE VARCHAR(16777216),
	START_TIME TIMESTAMP_LTZ(3),
	END_TIME TIMESTAMP_LTZ(3),
	TOTAL_ELAPSED_TIME NUMBER(38,0),
	BYTES_SCANNED NUMBER(38,0),
	ROWS_PRODUCED NUMBER(38,0),
	COMPILATION_TIME NUMBER(38,0),
	EXECUTION_TIME NUMBER(38,0),
	QUEUED_PROVISIONING_TIME NUMBER(38,0),
	QUEUED_REPAIR_TIME NUMBER(38,0),
	QUEUED_OVERLOAD_TIME NUMBER(38,0),
	TRANSACTION_BLOCKED_TIME NUMBER(38,0),
	OUTBOUND_DATA_TRANSFER_CLOUD VARCHAR(16777216),
	OUTBOUND_DATA_TRANSFER_REGION VARCHAR(16777216),
	OUTBOUND_DATA_TRANSFER_BYTES NUMBER(38,0),
	INBOUND_DATA_TRANSFER_CLOUD VARCHAR(16777216),
	INBOUND_DATA_TRANSFER_REGION VARCHAR(16777216),
	INBOUND_DATA_TRANSFER_BYTES NUMBER(38,0),
	CREDITS_USED_CLOUD_SERVICES NUMBER(38,9),
	LIST_EXTERNAL_FILE_TIME NUMBER(38,0),
	RELEASE_VERSION VARCHAR(16777216),
	EXTERNAL_FUNCTION_TOTAL_INVOCATIONS NUMBER(38,0),
	EXTERNAL_FUNCTION_TOTAL_SENT_ROWS NUMBER(38,0),
	EXTERNAL_FUNCTION_TOTAL_RECEIVED_ROWS NUMBER(38,0),
	EXTERNAL_FUNCTION_TOTAL_SENT_BYTES NUMBER(38,0),
	EXTERNAL_FUNCTION_TOTAL_RECEIVED_BYTES NUMBER(38,0),
	IS_CLIENT_GENERATED_STATEMENT BOOLEAN,
	QUERY_HASH VARCHAR(16777216),
	QUERY_HASH_VERSION NUMBER(38,0),
	QUERY_PARAMETERIZED_HASH VARCHAR(16777216),
	QUERY_PARAMETERIZED_HASH_VERSION NUMBER(38,0),
	constraint SQL_LOG_PK primary key (SQL_LOG_ID)
	//constraint TALEND_RUN_FK foreign key (RUN_ID) references TALEND_RUNS(RUN_ID)
);

//// note
-- if it's non-test env new project, create appropriate new roles first
-- if it's test env (e.g. DM_LADDERS_TEST)
-- go to worksheet ladders_db_grants
-- more info regarding the write permission for testing env is there
////

-- create task
use role accountadmin;

create or replace task DM_LADDERS_TEST.UTIL.S3_UNLOAD_TASK
	warehouse=COMPUTE_WH
	schedule='USING CRON 20 * * * * America/New_York'
	as Call metadata.procedures_dev.sp_data_unload('LADDERS_PAYLOAD_UPDATE', 'task_call_sp_data_unload', 'co-carecoordination-test', 'DM_LADDERS_TEST', 'UTIL', 'DM_LADDERS_TEST', 'UNLOAD_SF_TO_S3_GCP_CASE_EXTERNAL_ID|UNLOAD_SF_TO_S3_GCP_LOCATION_SITE_CODE|UNLOAD_SF_TO_S3_GCP_LOCATION|UNLOAD_SF_TO_S3_GCP|', null);

ALTER TASK DM_LADDERS_TEST.UTIL.S3_UNLOAD_TASK RESUME;
EXECUTE TASK DM_LADDERS_TEST.UTIL.S3_UNLOAD_TASK;

-- the following task is needed for the hades legacy and new bhe data integration --
-- before the integration is complete, the following task will be needed --
create or replace task DM_LADDERS_TEST_STAGING.UTIL.LADDERS_MAPPED_TABLE_TASK
	warehouse=COMPUTE_WH
	schedule='USING CRON 20 * * * * America/New_York'
	as BEGIN
    create or replace TRANSIENT TABLE DM_LADDERS_TEST_STAGING.DM.VW_LADDERS_MAPPED_INTEGRATION_TABLE as
        select * from DM_LADDERS_TEST_STAGING.DM.VW_LADDERS_MAPPED_INTEGRATION;
END;

ALTER TASK DM_LADDERS_QA.UTIL.LADDERS_MAPPED_TABLE_TASK RESUME;
EXECUTE TASK DM_LADDERS_QA.UTIL.LADDERS_MAPPED_TABLE_TASK;
-- end create task
